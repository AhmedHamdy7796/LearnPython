{"results": {"language_code": "en", "transcripts": [{"transcript": "[VIDEO PLAYBACK] - Since day one, we set out to\nsignificantly improve the lives of as many people as possible. And with a little help,\nyou found new answers, discovered new places. The right words came\nat just the right time, and we even learned how to\nspell the word \"epicurean.\" - R-I-A-N. - Life got a little easier. Our photos got a little better. And we got closer to a\nworld where we all belong. - All stations ready\nto resume count. 3, 2, 1, we have liftoff! - So as we stand on the cusp\nof a new era, new breakthroughs in AI, we'll reimagine\nthe ways we can help. We will have the chance to\nimprove the lives of billions of people. We will give businesses\nthe opportunity to thrive and grow and help\nsociety answer the toughest questions we have to face. Now, we don't take\nthis for granted. So while our ambition\nis bold, our approach will always be responsible,\nbecause our goal is to make AI helpful for everyone. [MUSIC PLAYING] [END PLAYBACK] [CHEERS, APPLAUSE] SUNDAR PICHAI: Good\nmorning, everyone. Welcome to Google I/O. [APPLAUSE] It's great to see so many\nof you here at Shoreline, so many developers. And a huge thanks\nto the millions joining from around the\nworld, from Bangladesh to Brazil to our new Bayview\ncampus right next door. It's so great to\nhave you, as always. As you may have heard, AI\nis having a very busy year, so we've got lots to talk about. Let's get started. Seven years into our journey\nas an AI-first company, we are at an exciting\ninflection point. We have an opportunity to\nmake AI even more helpful for people, for businesses,\nfor communities, for everyone. We have been applying AI to\nmake our products radically more helpful for a while. With generative AI, we\nare taking the next step. With a bold and\nresponsible approach, we are reimagining all our core\nproducts, including Search. You will hear more\nlater in the keynote. Let me start with a\nfew examples of how generative AI is helping\nto evolve our products, starting with Gmail. In 2017, we launched\nSmart Reply, short responses you could\nselect with just one click. Next came Smart Compose, which\noffered writing suggestions as you type. Smart Compose led to more\nadvanced writing features powered by AI. They've been used in Workspace\nover 180 billion times in the past year alone. And now, with a much more\npowerful generative model, we are taking the next step\nin Gmail with Help Me Write. Let's say you got this email\nthat your flight was canceled. The airline has sent a voucher,\nbut what you really want is a full refund. You could reply and\nuse Help Me Write. Just type in the\nprompt of what you want, an email to ask for\na full refund, hit Create, and a full draft appears. As you can see, it conveniently\npulled in flight details from the previous email. And it looks pretty close\nto what you want to send. Maybe you want to\ndefine it further. In this case, a\nmore elaborate email might increase the chances\nof getting the refund. [LAUGHTER] [APPLAUSE] And there you go. I think it's ready to send. Help Me Write will\nstart rolling out as part of our\nWorkspace updates. And just like with\nSmart Compose, you will see it get\nbetter over time. The next example is Maps. Since the early\ndays of Street View, AI have stitched together\nbillions of panoramic images so people can explore the\nworld from their device. At last year's I/O, we\nintroduced Immersive View, which uses AI to\ncreate a high fidelity representation of a place\nso you can experience it before you visit. Now, we are expanding\nthat same technology to do what maps does best-- help you get where\nyou want to go. Google Maps provides\n20 billion kilometers of directions every day. That's a lot of trips. Imagine if you could see\nyour whole trip in advance. With Immersive View\nfor Routes, now you can, whether you're walking,\ncycling, or driving. Let me show you what I mean. Say I'm in New York City and\nI want to go on a bike ride. Maps has given me a couple of\noptions close to where I am. I like the one on\nthe waterfront, so let's go with that. Looks scenic. I want to get a\nfeel for it first. Click on Immersive\nView for Route, and it's an entirely new\nway to look at my journey. I can zoom in to get an\nincredible bird's eye view of the ride. And as we turn, we get\nonto a great bike path. [APPLAUSE] It looks like it's going\nto be a beautiful ride. You can also check\ntoday's air quality. Looks like AQI is 43-- pretty good. And if I want to check\ntraffic and weather and see how they might change\nover the next few hours, I can do that. Looks like it's\ngoing to pour later. So maybe I want\nto get going now. Immersive View for\nRoutes will begin to roll out over the summer\nand launch in 15 cities by the end of the year,\nincluding London, New York, Tokyo, and San Francisco. [APPLAUSE] Another product made better\nby AI is Google Photos. We introduced it at I/O in 2015. It was one of our first\nAI-native products. Breakthroughs in\nmachine learning made it possible to search your\nphotos for things like people, sunsets, or waterfalls. Of course, we want you to do\nmore than just search photos. We also want to help\nyou make them better. In fact, every month,\n1.7 billion images are edited in Google Photos. AI advancements give us more\npowerful ways to do this. For example, Magic Eraser,\nlaunched first on Pixel, uses AI-powered\ncomputational photography to remove unwanted distractions. And later this year,\nusing a combination of semantic understanding\nand generative AI, you can do much more\nwith a new experience called Magic Editor. Let's have a look. Say you are on a\nhike and you stop to take a photo in\nfront of a waterfall. You wish you had taken\nyour bag off for the photo, so let's go ahead and\nremove that back strap. The photo feels a bit dark, so\nyou can improve the lighting. And maybe you want to even\nget rid of some clouds to make it feel as sunny\nas you remember it. Looking even\ncloser, you wish you had posed so it looks like\nyou're really catching the water in your hand. No problem-- you\ncan adjust that. [APPLAUSE] There you go. Let's look at one more photo. This is a great photo,\nbut as a parent, you always want your kid\nat the center of it all. And it looks like the balloons\ngot cut off in this one. So you can go ahead and\nreposition the birthday boy. Magic Editor\nautomatically recreates parts of the bench and\nballoons that were not captured in the original shot. As a finishing touch,\nyou can punch up the sky. It changes the lighting\nin the rest of the photo. So the Edit feels consistent. It's truly magical. We are excited to roll out\nMagic Editor in Google Photos later this year. From Gmail and\nPhotos to Maps, these are just a few examples of\nhow AI can help you in moments that matter. And there is so\nmuch more we can do to deliver the full\npotential of AI across the products\nyou know and love. Today, we have 15\nproducts that each serve more than half a\nbillion people and businesses. And six of those products sell\nover 2 billion users each. This gives us so\nmany opportunities to deliver on our mission,\nto organize the world's information, and\nmake it universally accessible and useful. It's a timeless\nmission that feels more relevant with\neach passing year. And looking ahead, making\nAI helpful for everyone is the most profound way we\nwill advance our mission. And we are doing this\nin four important ways. First, by improving\nyour knowledge and learning and deepening your\nunderstanding of the world. Second, by boosting\ncreativity and productivity so you can express yourself\nand get things done. Third, by enabling\ndevelopers and businesses to build their own\ntransformative products and services. And finally, by building and\ndeploying AI responsibly so that everyone can\nbenefit equally. We are so excited by\nthe opportunities ahead. Our ability to make AI\nhelpful for everyone relies on continuously\nadvancing our foundation models. So I want to take\na moment to share how we are approaching them. Last year, you heard\nus talk about PaLM, which led to many improvements\nacross our products. Today, we are ready to\nannounce our latest PaLM model in production, PaLM 2. [APPLAUSE] PaLM 2. This is one of our fundamental\nresearch and our latest infrastructure. It's highly capable at\na wide range of tasks, and easy to deploy. We are announcing over 25\nproducts and features powered by PaLM 2 today. PaLM 2 models deliver excellent\nfoundational capabilities across a wide range of sizes. We have affectionately named\nthem gecko, otter, bison, and unicorn. Gecko's so lightweight\nthat it can work on mobile\ndevices, fast enough or great interactive\napplications on device even when offline. PaLM 2 models are stronger\nin logic and reasoning thanks to broad training on scientific\nand mathematical topics. It's also trained on\nmultilingual text, spanning over 100 languages\nso to understand and generate nuanced results. Combined with powerful\ncoding capabilities, PaLM 3 can also help developers\ncollaborating around the world. Let's look at this example. Let's say you're working\nwith a colleague in Seoul and you're debugging code. You can ask it to fix\na bug and help out your teammate by adding\ncomments in Korean to the code. It first recognizes the code\nis recursive, suggests a fix, and even explains the\nreasoning behind the fix. And as you can see, it\nadded comments in Korean, just like you asked. [APPLAUSE] While PaLM 2 is highly\ncapable, it really shines when fine-tuned on\ndomain-specific knowledge. We recently released\nSec-PaLM, version of PaLM 2 fine-tuned for\nsecurity use cases. It uses AI to better\ndetect malicious scripts, and can help security experts\nunderstand and resolve threats. Another example is Med-PaLM 2. In this case, it's fine-tuned\non medical knowledge. This fine-tuning\nachieved a 9x reduction in inaccurate reasoning\nwhen compared to the model, approaching the performance\nof clinician experts who answered the same\nset of questions. In fact, Med-PaLM 2 was\nthe first language model to perform an expert level on\nmedical licensing exam-style questions, and is currently\nthe state of the art. We are also working to add\ncapabilities to Med-PaLM 2 so that it can\nsynthesize information from medical imaging, like\nplain films and mammograms. You can imagine\nan AI collaborator that helps radiologists\ninterpret images and communicate the results. These are some\nexamples of PaLM 2 being used in\nspecialized domains. We can't wait to\nsee it used in more. That's why I'm pleased to\nannounce that it is now available in Preview. And I'll let Thomas share more. [APPLAUSE] PaLM 2 is the latest step\nin our decade long journey to bring AI in responsible\nways to billions of people. It builds on progress made\nby two world-class teams, the Brain Team and DeepMind. Looking back at the\ndefining AI breakthroughs over the last\ndecade, these teams have contributed to a\nsignificant number of them-- AlphaGo, transformers,\nsequence-to-sequence models, and so on. All this helps set the stage\nfor the inflection point we are at today. We recently brought\nthese two teams together into a single unit,\nGoogle DeepMind. Using the computational\nresources of Google, they are focused on building\nmore capable systems safely and responsibly. This includes our\nnext-generation foundation model, Gemini, which\nis still in training. Gemini was created\nfrom the ground up to be multi-modal,\nhighly efficient at tool in API integrations, and built\nto enable future innovations, like memory and planning. While still early,\nwe are already seeing impressive multimodal\ncapabilities not seen in prior models. Once fine-tuned and\nrigorously tested for safety, Gemini will be available at\nvarious sizes and capabilities, just like PaLM 2. As we invest in more\nadvanced models, we are also deeply investing\nin AI responsibility. This includes having the tools\nto identify synthetically generated content\nwhenever you encounter it. Two important approaches are\nwatermarking and metadata. Watermarking embeds information\ndirectly into content in ways that are maintained even\nthrough modest image editing. Moving forward, we are\nbuilding our models to include watermarking\nand other techniques from the start. If you look at a\nsynthetic image, it's impressive\nhow real it looks. So you can imagine\nhow important this is going to be in the future. Metadata allows\ncontent creators to associate additional\ncontexts with original files, giving you more\ninformation whenever you encounter an image. We'll ensure every one of\nour AI-generated images has that metadata. James will talk about\nour responsible approach to AI later. As models get better\nand more capable, one of the most\nexciting opportunities is making them available for\npeople to engage with directly. That's the opportunity we\nhave at Bard, our experiment for conversational AI. We are rapidly evolving Bard. It now supports a wide range\nof programming capabilities, and it's gotten much smarter\nat reasoning and math problems. And as of today, it is now\nfully running on PaLM 2. To share more about\nwhat's coming, let me turn it over to Sissie. [MUSIC PLAYING] [APPLAUSE] SISSIE HSIAO: Thanks, Sundar. Large language models\nhave captured the world's imagination,\nchanging how we think about the future of computing. We launched Bard as a\nlimited-access experiment on a lightweight,\nlarge language model to get feedback and iterate. And since then, the team\nhas been working hard to make rapid improvements\nand launch them quickly. With PaLM 2, Bard's math,\nlogic, and reasoning skills made a huge leap forward,\nunderpinning its ability to help developers\nwith programming. Bard can now collaborate on\ntasks like code generation, debugging, and\nexplaining code snippets. Bard has already learned more\nthan 20 programming languages, including C++, Go,\nJavaScript, Python, Kotlin, and even Google\nSheets functions. And we're thrilled to see\nthat coding has quickly become one of the\nmost popular things that people are doing with Bard. So let's take a\nlook at an example. I've recently been\nlearning chess, and for fun, I\nthought I'd see if I can program a move in Python. How would I use Python to\ngenerate the \"scholar's mate\" move in chess? OK. Here, Bard created a script\nto recreate this chess move in Python. And notice how it also\nformatted the code nicely, making it easy to read. We've also heard great feedback\nfrom developers about how Bard provides code citations. And starting next week, you'll\nnotice something right here. We're making code citations\neven more precise. If Bard brings in\na block of code, just click this annotation, and\nBard will underline the block and link to the source. Now, Bard can also help\nme understand the code. Could you tell me what\nchess.board does in this code? Now, this is a super\nhelpful explanation of what it's doing and\nmakes things more clear. Let's see if we can make\nthis code a little better. How would I improve this code? Let's see. There's a list comprehension,\ncreating a function, and using a generator . Those are some\ngreat suggestions. Now, could you join them into\none single Python code block? Now, Bard is rebuilding the\ncode with these improvements. OK, great. How easy was that? And in a couple clicks, I can\nmove this directly into Colab. Developers love the ability\nto bring code from Bard into their workflow,\nlike to Colab. So coming soon, we're\nadding the ability to export and run code\nwith our partner Replit, starting with Python. [APPLAUSE] We've also heard that\nyou want dark theme, so starting today,\nyou can activate it. [APPLAUSE] You can activate\nit right in Bard or let it follow\nyour OS settings. And speaking of\nexporting things, people often ask\nBard for a head start drafting emails and documents. So today, we are launching\ntwo more export actions, making it easy to move\nBard's responses right into Gmail and Docs. [APPLAUSE] So we're excited by how quickly\nBard and the underlying models are improving, but we're\nnot stopping there. We want to bring more\ncapabilities to Bard to fuel your curiosity\nand imagination. And so I'm excited to announce\nthat tools are coming to Bard. [APPLAUSE] As you collaborate\nwith Bard, you'll be able to tap into\nservices from Google and extensions with partners to\nlet you do things never before possible. And of course, we'll\napproach this responsibly, in a secure and private\nway, letting you always stay in control. We're starting with\nsome of the Google Apps that people love\nand use every day. It's incredible what Bard\ncan already do with text, but images are such\na fundamental part of how we learn and express. So in the next few weeks,\nBard will become more visual, both in its responses\nand your prompts. So if you ask, what are some\nmust-see sites in New Orleans, Bard's going to use Google\nSearch and the knowledge graph to find the most\nrelevant images. Here we go. The French Quarter,\nthe Garden District. These images are really\ngiving me a much better sense of what I'm exploring. We'll also make it easy for\nyou to prompt Bard with images, giving you even more ways\nto explore and create. People love Google Lens,\nand in the coming months, we're bringing the\npowers of Lens to Bard. [APPLAUSE] So if you're looking to have\nsome fun with your fur babies, you might upload\nan image and ask Bard to write a funny\ncaption about these two. Lens detects that\nthis is a photo of a goofy German shepherd\nand a golden retriever, and then Bard uses that to\ncreate some funny captions. If you ask me, I think\nthey're both good boys. Now, Let's do another one. Imagine I'm 18 and I\nneed to apply to college. I won't date myself\nwith how long it's been, but it's still an\noverwhelming process. So I'm thinking about\ncolleges, but I'm not sure what I want to focus on. I'm into video games. And what kinds of programs\nmight be interesting? OK, this is a\nhelpful head start. Hmm, animation looks\npretty interesting. Now, I could ask,\nhelp me find colleges with animation programs\nin Pennsylvania. OK, great. That's a good list of schools. Now, to see where these\nare, I might now say, show these on a map. Here, Bard's going to use\nGoogle Maps to visualize where the schools are. [APPLAUSE] This is super helpful,\nand it's exciting to see that there's plenty of\noptions not too far from home. Now, let's start\norganizing things a bit. Show these options as a table. Nice-- structured and organized. But there's more I want to know. Add a column showing\nwhether they're public or private schools. [APPLAUSE] Perfect. This is a great\nstart to build on. And now, let's move\nthis to Google Sheets so my family can jump in later\nto help me with my search. [APPLAUSE] You can see how easy it will\nbe to get a jump start in Bard and quickly have something\nuseful to move over to apps like Docs or Sheets\nto build on with others. OK, now that's a taste of\nwhat's possible when Bard meets some of Google's apps. But that's just the start. Bard will be able to tap\ninto all kinds of services from across the\nweb, with extensions from incredible\npartners like Instacart, Indeed, Khan Academy,\nand many more. So here's a look at one coming\nin the next couple of months. With Adobe Firefly,\nyou'll be able to generate completely new images from\nyour imagination right in Bard. Now, let's say I'm\nplanning a birthday party for my seven-year-old,\nwho loves unicorns. I want a fun image to send\nout with the invitations. Make an image of a unicorn\nand a cake at a kid's party. Now, Bard is\nworking with firefly to bring what I\nimagined to life. [APPLAUSE] How amazing is that? This will unlock\nall kinds of ways that you can take your\ncreativity further and faster. And we are so excited\nfor this partnership. Bard continues to\nrapidly improve and learn new abilities, and we want to\nlet people around the world try it out and share\ntheir feedback. So today, we are\nremoving the waitlist and opening up Bard to over\n180 countries and territories-- [APPLAUSE] --with more coming soon. And in addition to becoming\navailable in more places, Bard is also becoming\navailable in more languages. Beyond English,\nstarting today, you'll be able to talk to Bard\nin Japanese and Korean. Adding languages responsibly\ninvolves deep work to get things like quality\nand local nuances right. And we're pleased to share\nthat we're on track to support 40 languages soon. [APPLAUSE] It's amazing to see the\nrate of progress so far-- more advanced models, so\nmany new capabilities, and the ability for\neven more people to collaborate with Bard. And when we're ready to move\nBard to our Gemini model, I'm really excited about\nmore advancements to come. So that's where we're going\nwith Bard, connecting tools from Google and amazing\nservices across the web to help you do and create\nanything you can imagine through a fluid\ncollaboration with our most capable, large language models. There's so much to\nshare in the days ahead. And now, to hear more about\nhow large language models are enabling next-generation\nproductivity features right in Workspace, I'll\nhand it over to Aparna. [MUSIC PLAYING] [APPLAUSE] APARNA PAPPU: From\nthe very beginning, Workspace was built to allow\nyou to collaborate in real time with other people. Now, you can collaborate\nin real time with AI. AI can act as a coach,\na thought partner, a source of\ninspiration, as well as a productivity booster across\nall of the apps of Workspace. Our first steps with\nAI as a collaborator were via the Help Me Write\nfeature in Gmail and Docs, which launched to\ntrusted testers in March. We've been truly blown away by\nthe clever and creative ways these features are being\nused, from writing essays, sales pitches, project\nplans, client outreach, and so much more. Since then, we've\nbeen busy expanding these helpful features\nacross more surfaces. Let me show you a few examples. One of our most\npopular use cases is the trusty job description. Every business, big or\nsmall, needs to hire people. A good job description can\nmake all the difference. Here's how Docs\nhas been helping. Say you run a fashion\nboutique and need to hire a textile designer. To get started, you enter\njust a few words as a prompt. Senior-level job description\nfor textile designer. Docs will take that prompt,\nsend it to a PaLM 2-based model, and let's see what I got back. Not bad. With just seven words,\nthe model came back with a good starting\npoint written out really nicely for me. Now, you can take\nthat and customize it for the kind of experience,\neducation, and skill set that this role needs, saving\nyou a ton of time and effort. Next-- [APPLAUSE] --let me show you how you can\nget more organized with Sheets. Imagine you run a\ndog walking business and need to keep track of things\nlike your clients, logistics about the dogs, like what\ntime they need to be walked, for how long, et cetera. Sheets can help\nyou get organized. In a new sheet,\nsimply type something like \"client and pet roster\nfor a dog walking business with rates\" and hit Create. Sheets sends this input\nto a fine-tuned model that we've been training with\nall sorts of Sheets-specific use cases. Look at that. The model-- [APPLAUSE] The model figured out\nwhat you might need. The generated table has\nthings like the dog's name, client info, notes, et cetera. This is a good start\nfor you to tinker with. Sheets made it\neasy for you to get started so you can go back\nto doing what you love. Speaking of getting\nback to things you love, let's talk\nabout Google Slides. People use Slides for\nstorytelling all the time, whether at work or in\ntheir personal lives. For example, you get\nyour extended family to collect anecdotes,\nhaikus, jokes for your parents' 50th wedding\nanniversary in a slide deck. Everyone does their bit. But maybe this deck\ncould have more pizzazz. Let's pick one of the slides\nand use the poem on there as a prompt for\nimage generation. \"Mom loves her pizza,\ncheesy and true, while dad's favorite treat\nis a warm pot of fondue.\" Let's hit Create and see\nwhat it comes up with. Behind the scenes, that quote\nis sent as an input to our text to image models. And we know it's unlikely\nthat the user will be happy with just one option,\nso we generate about six to eight images so that\nyou have the ability to choose and refine. Whoa, I have some oddly\ndelicious-looking fondue pizza images. Now, this style is a\nlittle too cartoony for me. So I'm going to ask\nit to try again. Let's change the\nstyle to photography and give it a whirl. Just as weird, but\nit works for me. You can have endless\nfun with this, with no limits on\ncheesiness or creativity. Starting next month,\ntrusted testers will be able to try this and\nsix more generative AI features across Workspace. And later this year,\nall of this will be generally available\nto business and consumer Workspace users via\na new service called Duet AI for Workspace. [APPLAUSE] Stepping back a bit, I showed\nyou a few powerful examples of how Workspace can help\nyou get more done with just a few words as prompts. Prompts are a powerful way\nof collaborating with AI. The right prompt can unlock\nfar more from these models. However, it can be\ndaunting for many of us to even know where to start. Well, what if we could\nsolve that for you? What if AI could proactively\noffer you prompts? Even better, what if these\nprompts were actually contextual and changed based\non what you are working on? I am super excited to show\nyou a preview of just that. This is how we see the\nfuture of collaboration with AI coming to life. Let's switch to a live\ndemo so I can show you what I mean Tony's here\nto help me with that. Hey, Tony. TONY: Hey, Aparna. APARNA PAPPU: So-- [APPLAUSE] My niece Mira and I are\nworking on a spooky story together for summer camp. We've already written\na few paragraphs. But now, we're stuck. Let's get some help. As you can see, we launch a\nside panel, something the team fondly calls Sidekick. Sidekick instantly reads\nand processes the document and offers some really\nneat suggestions, along with an open\nprompt dialog. If we look closely, we can\nsee some of the suggestions, like what happened to\nthe golden seashell? What are common\nmystery plot twists? Let's try the seashell\noption and see what it comes back with. Now, what's happening\nbehind the scenes is that we've provided\nthe entire document as context to the model, along\nwith the suggested prompt. And let's see what we got back. The golden seashell was\neaten by a giant squid that lives in the cove. This is a good start. Let's insert these\nnotes so that we can continue our little project. Now, one of the interesting\nobservations we have is that it's actually easier\nto react to something, or perhaps use that\nto say, hmm, I want to go in a different direction. And this is exactly\nwhat AI can help with. I see a new suggestion on\nthere for generating images. Let's see what this does. The story has a village,\na golden seashell, and other details. And instead of having\nto type all of that out, the model picks up these\ndetails from the document and generates images. These are some cool\npictures, and I bet my niece will love these. Let's insert them\ninto the dock for fun. Thank you, Tony. [APPLAUSE] I'm going to walk you\nthrough some more examples, and this will help you see why\nthis powerful new contextual collaboration is such\na remarkable boost to productivity and creativity. Say you're writing\nto your neighbors about an upcoming potluck. Now, as you can see,\nSidekick has summarized what this conversation is about. Last year, everyone\nbrought hummus. Who doesn't love hummus? But this year, you want\na little more variety. Let's see what people\nsigned up to bring. Well, somewhere in this thread\nis a Google Sheet where you've collected that information. You can get some help\nby typing, \"write a note about the main\ndishes people are bringing.\" And let's see what we get back. Let's go. Awesome. It found the right Sheet and\ncited the source in the Found In section, giving\nyou confidence that this is not made up. It looks good. You can insert it\ndirectly into your email. Let's end with an example of\nhow this can help you at work. Say you're about to give\nan important presentation, and you've been so\nfocused on the content that you forgot to\nprepare speaker notes. The presentation is in an hour. Uh-oh. No need to panic. Look at what one of\nthe suggestions is. Create speaker notes\nfor each slide. Let's see what happened. [APPLAUSE] What happened behind\nthe scenes here is that the presentation\nand other relevant context was sent to the model to\nhelp create these notes. And once you've\nreviewed them, you can hit Insert\nand edit the notes to convey what you intended. So you can now deliver\nthe presentation without worrying\nabout the notes. As you can see,\nwe've been having a ton of fun playing with this. We can see the true potential\nof AI as a collaborator. And we'll be bringing\nthis experience to Duet AI for Workspace. With that, I'll hand\nit back to Sundar. [MUSIC PLAYING] [APPLAUSE] SUNDAR PICHAI: Thanks, Aparna. It's exciting to see\nall the innovation coming to Google Workspace. As AI continues to\nimprove rapidly, we have focused on giving\nhelpful features to our users. And starting today, we\nare giving you a new way to preview some\nof the experiences across Workspace\nand other products. It's called Labs. I say \"new,\" but Google has a\nlong history of bringing Labs, and we've made it available\nthroughout our history as well. You can check it out\nat google.com/labs. Next up, we're going\nto talk about Search. Search has been our founding\nproduct from our earliest days, and we always approached\nit placing user trust above everything else. To give you a sense\nof how we are bringing generative AI in\nSearch, I'm going to invite Cathy onto the stage. Cathy? [APPLAUSE] [MUSIC PLAYING] CATHY EDWARDS: Thanks, Sundar. I've been working in\nSearch for many years. And what inspires me so\nmuch is how it continues to be an unsolved problem. And that's why I'm just so\nexcited by the potential of bringing generative\nAI into Search. Let's give it a whirl. So let's start with\na search for \"what's better for a family\nwith kids under three and a dog, Bryce\nCanyon or Arches?\" Now, although this is the\nquestion that you have, you probably wouldn't\nask it in this way today. You'd break it down\ninto smaller ones, sift through the information,\nand then piece things together yourself. Now, Search does the\nheavy lifting for you. What you see here\nlooks pretty different. So let me first give\nyou a quick tour. You'll notice the new,\nintegrated search results page so you can get even more\nout of a single search. There's an AI-powered snapshot\nthat quickly gives you the lay of the land on a topic. And so here you can see\nthat while both parks are kid-friendly, only\nBryce Canyon has more options for your furry friend. Then if you want to\ndig deeper, there are links included\nin the snapshot. You can also click\nto expand your view. And you'll see how the\ninformation is corroborated so you can check\nout more details and really explore the\nrichness of the topic. This new experience builds on\nGoogle's ranking and safety systems that we've been\nfine-tuning for decades. And Search will continue to be\nyour jumping-off point to what makes the web so special,\nits diverse range of content, from publishers to\ncreatives, businesses, and even people like you and me. So you can check\nout recommendations from experts, like the\nNational Park Service, and learn from authentic,\nfirsthand experiences, like the Mom Trotter Blog,\nbecause even in a world where AI can provide insights, we know\nthat people will always value the input of other people. And a thriving web\nis essential to that. These new generative AI-- thank you. [APPLAUSE] These new generative\nAI capabilities will make such smarter\nand searching simpler. And as you've seen, this is\nreally especially helpful when you need to make\nsense of something complex, with multiple angles to explore. You know, those times when even\nyour question has questions. So for example, let's\nsay you're searching for a good bike for a\nfive-mile commute with hills. This can be a big purchase, so\nyou want to do your research. In the AI-powered\nsnapshot, you'll see important considerations\nlike motor and battery for taking on those\nhills and suspension for a comfortable ride. Right below that,\nyou'll see products that fit the bill,\neach with images, reviews, helpful descriptions,\nand current pricing. This is built on\nGoogle's Shopping graph, the world's most\ncomprehensive data set of constantly changing\nproducts, sellers, brands, reviews, and\ninventory out there, with over 35 billion listings. In fact, there are 1.8 billion\nlive updates to our Shopping graph every hour. So you can shop with confidence\nin this new experience, knowing that you'll get\nfresh, relevant results. And for commercial\nqueries like this, we also know that\nads can be especially helpful to connect people\nwith useful information and help businesses\nget discovered online. They're here, clearly labeled. And we're exploring\ndifferent ways to integrate them as we roll\nout new experiences in Search. And now that you've\ndone some research, you might want to explore more. So right under the\nsnapshot, you'll see the option to ask\na follow-up question or select a suggested next step. Tapping any of these\noptions will bring you into our brand-new\nconversational mode. [APPLAUSE] In this case, maybe you want to\nask a follow-up about e-bikes, so you look for one in\nyour favorite color-- red. And without having to\ngo back to square one, Google Search understands\nyour full intent and that you're\nlooking specifically for e-bikes in red that would\nbe good for a five-mile commute with hills. And even when you're in\nthis conversational mode, it's an integrated experience. So you can simply scroll to\nsee other search results. Now, maybe this e-bike seems to\nbe a good fit for your commute. With just a click,\nyou're able to see a variety of retailers\nthat have it in stock, and some that offer free\ndelivery or returns. You'll also see current\nprices, including deals, and can seamlessly go\nto a merchant site, check out, and\nturn your attention to what really matters-- getting ready to ride. These new generative\nAI capabilities also unlock a whole new category\nof experiences on Search. It could help you create a\nclever name for your cycling club, craft the perfect\nsocial post to show off your new wheels, or even test\nyour knowledge on bicycle hand signals. These are things you\nmay never have thought to ask Search for before. Shopping is just one example\nof where this can be helpful. Let's walk through another\none in a live demo. What do you say? [APPLAUSE] Yeah. So special shout-out to my\nthree-year-old daughter, who is obsessed with whales. I wanted to teach\nher about whale song. So let me go to the\nGoogle app and ask, why do whales like to sing? And so here, I see a snapshot\nthat organizes the web results and gets me to key things I\nwant to know so I can understand quickly that, oh, they sing\nfor a lot of different reasons, like to communicate with other\nwhales, but also to find food. And I can click See More\nto expand here as well. Now, if I was actually with\nmy daughter and not on stage in front of thousands\nof people, I'd be checking out some of\nthese web results right now. They look pretty good. Now, I'm thinking\nshe'd get a kick out of seeing one up close. So let me ask, can I see\nwhales in California? And so the LLMs right now\nare working behind the scenes to generate my\nsnapshot, distilling insights and perspectives\nfrom across the web. It looks like, in\nnorthern California, I can see humpbacks\naround this time of year. That's cool. I'll have to plan to\ntake her on a trip soon. And again, I can see\nsome really great results from across the web. And if I want to\nrefer to the results of my previous question, I\ncan just scroll right up. Now, she's got a\nbirthday coming up, so I can follow up with \"plush\nones for kids under $40.\" Again, the LLMs are organizing\nthis information for me. And this process will\nget faster over time. These seem like\nsome great options. I think she'll really\nlike the second one. She's into orcas as well. Phew. Live demos are always\na little nerve-racking. I'm really glad that\none went \"whale.\" [APPLAUSE] What you've seen today\nis just a first look at how we're experimenting\nwith generative AI in Search. And we're excited to keep\nimproving with your feedback through our Search Labs program. This new Search generative\nexperience, also known as SGE, will be available in Labs, along\nwith some other experiments. And they'll be rolling\nout in the coming weeks. If you're in the US, you\ncan join the waitlist today by tapping the Labs icon in the\nlatest version of the Google app or Chrome desktop. This new experience really\nreflects the beginning of a new chapter. And you can think of\nthis evolution as Search, supercharged. Search has been at the core\nof our timeless mission for 25 years. And as we build for\nthe future, we're so excited for you to\nturn to Google for things you never dreamed it could. Here's an early look at what's\nto come for AI in Search. [VIDEO PLAYBACK] [YUNG CXREAL & BABY\nFRANKIE, \"LIKE WHOA\"] - Yes, yes, yes. - You got this. Let's go. - Is a hot dog sandwich? And the answer is-- - Yes. - No. - Yes! - No! [END PLAYBACK] [APPLAUSE] SUNDAR PICHAI: Is a\nhot dog a sandwich? I think it's more like a\ntaco because the bread goes around it. Comes from the expert\nviewpoint of a vegetarian. Thanks, Cathy. It's so exciting\nto see how we are evolving Search,\nand look forward to building it with you all. So far today, we\nhave shared how AI can help unlock creativity,\nproductivity, and knowledge. As you can see, AI is not\nonly a powerful enabler. It's also a big platform shift. Every business and\norganization is thinking about how to\ndrive transformation. That's why we are focused on\nmaking it easy and scalable for others to innovate with AI. That means providing the\nmost advanced computing infrastructure, including\nstate-of-the-art TPUs and GPUs, and expanding access to Google's\nlatest foundation models that have been rigorously\ntested in our own products. We are also working to\nprovide world-class tooling so customers can train,\nfine-tune, and run their own models with\nenterprise-grade safety, security, and privacy. To tell you more about how\nwe are doing this with Google Cloud, please welcome Thomas. [MUSIC PLAYING] [APPLAUSE] THOMAS KURIAN: All\nof the investments you've heard about today are\nalso coming to businesses. So whether you're an\nindividual developer or a full-scale enterprise,\nGoogle is using the power of AI to transform the way you work. There are already\nthousands of companies using our generative AI platform\nto create amazing content, to synthesize and\norganize information, to automate processes, and\nto build incredible customer experiences. And yes, each and every\none of you can too. There are three\nways Google Cloud can help you take advantage\nof the massive opportunity in front of you. First, you can build\ngenerative applications using our AI platform Vertex AI. With Vertex, you can\naccess foundation models for chat, text, and image. You just select the\nmodel you want to use, create prompts to\ntune the model, and you can even fine-tune\nthe model's weights on your own dedicated\ncompute clusters. To help you retrieve fresh\nand factual information from your company's databases,\nyour corporate intranet, your website, and\nenterprise applications, we offer Enterprise Search. Our AI platform is so\ncompelling for businesses because it guarantees\nthe privacy of your data. With both Vertex and\nEnterprise Search, you have sole\ncontrol of your data and the costs of using\ngenerative AI models. In other words, your data is\nyour data and no one else's. You can also choose the best\nmodel for your specific needs across many sizes that have\nbeen optimized for cost, latency, and quality. Many leading companies are using\nour alternative AI technologies to build super\ncool applications, and we've all been blown\naway by what they're doing. Let's hear from a few of them. [VIDEO PLAYBACK] [MUSIC PLAYING] - The unique thing\nabout Google Cloud is the expansive offering. - The Google\npartnership has taught us to lean in, to iterate,\nto test and learn, and have the courage to fail\nfast where we need to. - But also, Google's\nreally AI-centric company. And so there's a lot\nfor us to learn directly from the engineering team. - Now, with generative AI,\nwe can have much smarter conversations with\nour customers. - We have been really enjoying\ntaking the latest and greatest technology and making\nthat accessible to our entire community. - Getting early\naccess to Vertex APIs opens a lot of\ndoors for us to be more efficient and productive\nin the way we create experiences for our customers. - The act of making software\nis really suddenly opened up to everyone. Now, you can talk to the AI\non the Replit app and tell it, make me a workout program. And with one click, we can\ndeploy it to a Google Cloud VM, and you have an app that you\njust talked into existence. - We have an extraordinarily\nexciting feature in the pipeline. It's called Magic Video, and it\nenables you to take your videos and images, and with\njust a couple of clicks, turn that into a cohesive story. It is powered by\nGoogle's PaLM technology, and it truly empowers\neveryone to be able to create a video\nwith absolute ease. - Folks come to a Wendy's,\nand a lot of times, they use some of our acronyms. The junior bacon\ncheeseburger, they'll come in and-- give me a JBC. We need to understand\nwhat that really means. And we say, I can help make\nsure that order is accurate every single time. - Generative AI\ncan be incorporated in all the business processes\nDeutsche Bank is running. - The partnership with\nGoogle has inspired us to leverage technology to truly\ntransform the whole restaurant experience. - There is no limitations. - There's no other\nway to describe it. We're just living in the future. [END PLAYBACK] [APPLAUSE] We're also doing this with\npartners, like character.ai. We provide Character\nwith the world's most performant and\ncost-efficient infrastructure for training and\nserving the models. By combining its\nown AI capabilities with those of Google\nCloud, consumers can create their own deeply\npersonalized characters and interact with them. We're also partnering\nwith Salesforce to integrate Google Cloud's\nAI models and BigQuery with their data Cloud in\nEinstein, their AI-infused CRM Assistant. In fact, we're working with\nmany other incredible partners, including consultancies,\nsoftware-as-a-service leaders, consumer internet companies, and\nmany more to build remarkable experiences with\nour AI technologies. In addition to PaLM 2,\nwe're excited to introduce three new models in Vertex,\nincluding Imagen, which powers image generation, editing,\nand customization from text inputs, Codey for code\ncompletion and generation, which you can train\non your own code base to help you build\napplications faster, and Chirp, our universal\nspeech model which brings speech-to-text accuracy\nfor over 300 languages. We're also introducing\nreinforcement learning from human\nfeedback into Vertex AI. You can fine-tune\npre-trained models by incorporating human\nfeedback to further improve the model's results. You can also fine-tune\na model on domain- or industry-specific data, as we\nhave what Sec-PaLM and Med-PaLM so they become\neven more powerful. All of these features\nare now in Preview, and I encourage each and\nevery one of you to try them. [APPLAUSE] The second way we're\nhelping you take advantage of this opportunity\nis by introducing Duet AI for Google Cloud. Earlier, Aparna told you about\nDuet AI for Google Workspace and how it is an\nalways-on AI collaborator to help people get things done. Well, the same thing\nis true with Duet AI for Google Cloud, which\nserves as an AI expert pair programmer. Duet uses generative AI to\nprovide developers assistance, wherever you need it, within\nthe IDE, the Cloud Console, or directly within chat. It can provide you\ncontextual code completion, offer suggestions tuned\nto your code base, and generate entire\nfunctions in real time. It can even assist you with code\nreviews and code inspection. Hen will show you more\nin the developer keynote. The third way we're helping\nyou seize this moment is by building all\nof these capabilities on our AI-optimized\ninfrastructure. This infrastructure makes\nlarge-scale training workloads up to 80% faster and\nup to 50% cheaper compared to any\nalternatives out there. Look-- when you nearly\ndouble performance-- [APPLAUSE] When you nearly\ndouble performance for less than half the\ncost, amazing things happen. Today, we're excited to\nannounce a new addition to this infrastructure family,\nthe A3 Virtual Machines, based on Nvidia's\nlatest H100 GPUs. We provide the widest\nchoice of compute options for leading AI companies,\nlike Entropik and Midjourney, to build their future\non Google Cloud. And yes, there's so\nmuch more to come. Next, Josh is here to\nshow you exactly how we're making it\neasy and scalable for every developer to\ninnovate with AI and PaLM 2. [MUSIC PLAYING] [APPLAUSE] JOSH WOODWARD: Thanks, Thomas. Our work is enabling\nbusinesses, and it's also empowering developers. PaLM 2, our most\ncapable language model that Sundar talked about,\npowers the PaLM API. Since March, we've been running\na private preview with our PaLM API, and it's been amazing to\nsee how quickly developers have used it in their applications. Like Chaptr, who are\ngenerating stories so you can choose\nyour own adventure, forever changing story time. Or Game On Technology,\na company that makes chat apps for sports\nfans and retail brands to connect with their audiences. And there's also Wendy's. They're using the PaLM\nAPI to help customers place that correct order\nfor the junior bacon cheeseburger they talked about\nin their Talk to Menu feature. But I'm most excited\nabout the response we've gotten from the\ndeveloper tools community. Developers want choice when\nit comes to language models, and we're working with leading\ndeveloper tools companies, like LangChain, Chroma, and many\nmore to support the PaLM API. We've also integrated it\ninto Google Developer tools like Firebase and Colab. [APPLAUSE] You can hear a lot more about\nthe PaLM API in the developer keynote and sign up today. Now, to show you just how\npowerful the PaLM API is, I want to share one concept\nthat five engineers at Google put together over\nthe last few weeks. The idea is called\nProject Tailwind, and we think of it as\nan AI-first notebook that helps you learn faster. Like a real notebook, your\nnotes and your sources power Tailwind. How it works is you can simply\npick the files from Google Drive and it effectively creates\na personalized and private AI model that has expertise in the\ninformation that you give it. We've been developing\nthis idea with authors, like Stephen Johnson, and\ntesting it at universities, like Arizona State and the\nUniversity of Oklahoma, where I went to school. You want to see how it works? [APPLAUSE] Let's do a live demo. Now, imagine I'm a student\ntaking a computer science history class. I'll open up Tailwind,\nand I can quickly see, in Google Drive, all my\ndifferent notes and assignments and readings. I can insert them. And what will happen when\nTailwind loads up is you can see my different notes\nand articles on the side. Here they are, in the middle. And it instantly\ncreates a study guide on the right to\ngive me bearings. You can see it's pulling out key\nconcepts and questions grounded in the materials\nthat I've given it. Now, I can come over\nhere and quickly change it to go across all\nthe different sources and type something like \"create\nglossary for Hopper.\" And what's going to\nhappen behind the scenes is it'll automatically\ncompile a glossary associated with all the different\nnotes and articles relating to Grace Hopper,\nthe computer science history pioneer. Look at this-- FLOW-MATIC,\nCOBOL, compiler, all created based on my notes. Now, let's try one more. I'm going to try something else\ncalled \"different viewpoints on Dynabook.\" So the Dynabook, this was\na concept from Alan Kay. Again, Tailwind\ngoing out, finding all the different things. You can see how\nquick it comes back. There it is. And what's interesting\nhere is it's helping me think through the concept. So it's giving me\ndifferent viewpoints. It was a visionary product. It was a missed opportunity. But my favorite part\nis it shows its work. You can see the citations here. When I hover over, here's\nsomething from my class notes. Here's something from an\narticle the teacher assigned. It's all right here,\ngrounded in my sources. [APPLAUSE] Now, project Tailwind is\nstill in its early days, but we've had so much fun\nmaking this prototype, and we realized, it's\nnot just for students. It's helpful for anyone\nsynthesizing information from many different sources\nthat you choose, like writers researching an\narticle, or analysts going through earnings\ncalls, or even lawyers preparing for a case. Imagine collaborating\nwith an AI that's grounded in what you've\nread in all of your notes. We want to make it\navailable to try it out if you want to see it. [APPLAUSE] There's a lot more\ncan do with PaLM 2. And we can't wait to see what\nyou build using the PaLM API. Generative AI is\nchanging what it means to develop new products. At Google, we offer the\nbest ML infrastructure, with powerful models,\nincluding those in Vertex, and the APIs and tools\nto quickly generate your own applications. Building bold AI requires\na responsible approach. So let me hand it over\nto James to share more. Thanks. [MUSIC PLAYING] [APPLAUSE] JAMES MANYIKA: Hi, everyone. I'm James. In addition to research, I\nlead a new area at Google called technology and society. Growing up in\nZimbabwe, I could not have imagined all the amazing\nand groundbreaking innovations that have been presented\non this stage today. And while I feel it's\nimportant to celebrate the incredible progress in\nAI and the immense potential that it has for\npeople in society everywhere, we must\nalso acknowledge that it's an emerging technology\nthat is still being developed, and there's still\nso much more to do. Earlier, you heard Sundar\nsay that our approach to AI must be both bold\nand responsible. While there's a natural\ntension between the two, we believe it's\nnot only possible, but in fact critical to embrace\nthat tension productively. The only way to be truly\nbold in the long term is-- tension productively. The only way to be truly\nbold in the long term is to be responsible\nfrom the start. Our field-defining research\nis helping scientists make bold advances in\nmany scientific fields, including medical breakthroughs. Take, for example, Google\nDeepMind's AlphaFold, which can accurately\npredict the 3D shapes of 200 million proteins. That's nearly all the catalog\nproteins known to science. AlphaFold gave us the equivalent\nof nearly 400 million years of progress in just weeks. [APPLAUSE] So far, more than one million\nresearchers around the world have used AlphaFold's\npredictions, including Fang Zheng's\npioneering lab at the Broad Institute of MIT and Harvard. [APPLAUSE] Yeah. In fact, in March this year,\nZheng and his colleagues at MIT announced that\nthey'd used AlphaFold to develop a novel molecular\nsyringe which could deliver drugs to help improve the\neffectiveness of treatments for diseases like cancer. [APPLAUSE] And while it's\nexhilarating to see such bold and beneficial\nbreakthroughs, AI also has the potential\nto worsen existing societal challenges, like unfair bias,\nas well as pose new challenges as it becomes more advanced\nand new users emerge. That's why we believe\nit's imperative to take a responsible approach to AI. This work centers\naround our AI principles that we first\nestablished in 2018. These principles guide\nproduct development, and they help us assess\nevery AI application. They prompt questions like,\nwill it be socially beneficial, or could it lead\nto harm in any way? One area that is top of mind\nfor us is misinformation. Generative AI makes\nit easier than ever to create new\ncontent, but it also raises additional questions\nabout its trustworthiness. That's why we're developing\nand providing people with tools to evaluate online information. For example, have you come\nacross a photo on a website, or one shared by a friend,\nwith very little context, like this one of\nthe moon landing, and found yourself\nwondering, is this reliable? I have, and I'm sure\nmany of you have as well. In the coming months, we're\nadding two new ways for people to evaluate images. First, with our About this\nImage tool in Google Search, you'll be able to see important\ninformation, such as when and where similar images\nmay have first appeared, where else the image\nhas been seen online, including news, fact\nchecking, and social sites, all this providing you\nwith helpful context to determine if it's reliable. Later this year, you'll\nalso be able to use it if you search for an image or\nscreenshot using Google Lens or when you're on\nwebsites in Chrome. As we begin to roll out the\ngenerative image capabilities, like Sundar mentioned,\nwe will ensure that every one of our\nAI-generated images has metadata and markup\nin the original file to give you context\nif you come across it outside of our platforms. Not only that--\ncreators and publishers will be able to add\nsimilar metadata, so we'll be able to\nsee a label in images in Google Search marking\nthem as AI-generated. [APPLAUSE] As we apply our AI\nprinciples, we also start to see potential\ntensions when it comes to being bold and responsible. Here's an example. Universal Translator is an\nexperimental video dubbing service that helps experts\ntranslate a speaker's voice while also matching\ntheir lip movements. Let me show you how it\nworks with an online college course created in partnership\nwith Arizona State University. What many college\nstudents don't realize is that knowing\nwhen to ask for help and then following through\non using helpful resources is actually a hallmark of\nbecoming a productive adult. [SPEAKING SPANISH] [APPLAUSE] Yeah, it's cool. We use next-generation\ntranslation models to translate what the\nspeaker is saying, models to replicate\nthe style and the tone, and then match the\nspeaker's lip movements. Then we bring it all together. This is an enormous step forward\nfor learning comprehension. And we are seeing promising\nresults of course completion rates. But there's an\ninherent tension here. You can see how this can\nbe incredibly beneficial, but some of the same\nunderlying technology could be misused by bad\nactors to create deep fakes. So we built this\nservice with guardrails to help prevent misuse and\nto make it accessible only to authorized partners. [APPLAUSE] And as Sundar\nmentioned, soon, we'll be integrating new innovations\nin watermarking into our latest generative models to also\nhelp with the challenge of misinformation. Our AI principles also help\nguide us on what not to do. For instance, years ago, we\nwere the first major company to decide not to make a\ngeneral-purpose facial recognition API\ncommercially available. We felt there weren't\nadequate safeguards in place. Another way we live up\nto our AI principles is with innovations\nto tackle challenges as they emerge, like reducing\nthe risk of problematic outputs that may be generated\nby our models. We are one of the first in the\nindustry to develop and launch automated adversarial\ntesting using large language model technology. We do this for queries like\nthis to help uncover and reduce inaccurate outputs, like\nthe one on the left, and make them better,\nlike the one on the right. We're doing this at a scale\nthat's never been done before at Google, significantly\nimproving the speed, quality, and coverage of\ntesting, allowing safety experts to focus on\nthe most difficult cases. And we're sharing these\ninnovations with others. For example, our perspective\nAPI, originally created to help publishers\nmitigate toxicity, is now being used in\nlarge language models. Academic researchers have\nused our perspective API to create an industry\nevaluation standard. And today, all significant\nlarge language models, including those from\nOpenAI and Entropik, incorporate this standard\nto evaluate toxicity generated by their own models. Building AI-- building-- sorry. [APPLAUSE] Building AI responsibly must be\na collective effort involving researchers, social scientists,\nindustry experts, governments, and everyday people, as well\nas creators and publishers. Everyone benefits from a\nvibrant content ecosystem, today and in the future. That's why we're\ngetting feedback and we'll be working with\nthe web community on ways to give publishers choice and\ncontrol over their web content. It's such an exciting time. There's so much we're\ngoing to accomplish and so much we must\nget right together. We look forward to\nworking with all of you. And now, I'll hand\nit off to Sameer, who will speak to you about all\nthe exciting developments we're bringing to Android. Thank you. [APPLAUSE] SAMEER SAMAT: Hi, everyone. It's great to be back at Google\nI/O. As you've heard today, our bold and responsible\napproach to AI can unlock people's\ncreativity and potential. But how can all this\nhelpfulness reach as many people as possible? At Google, our computing\nplatforms and hardware products have been integral\nto that mission. From the beginning\nof Android, we believed that an open OS\nwould enable a whole ecosystem and bring smartphones\nto everyone. And as we all add more devices\nto our lives, like tablets, TVs, cars, and\nmore, this openness creates the freedom to\nchoose the devices that work best for you. With more than 3\nbillion Android devices, we've now seen the benefits of\nusing AI to improve experiences at scale. For example, this\npast year, Android used AI models to protect\nusers for more than 100 billion suspected spam\nmessages and calls. [APPLAUSE] We can all agree\nthat's pretty useful. There are so many\nopportunities where AI can just make things better. Today, we'll talk\nabout two big ways Android is bringing that benefit\nof computing to everyone. First, continuing to connect you\nto the most complete ecosystem of devices, where everything\nworks better together. And second, using AI\nto make the things you love about Android even better,\nstarting with customization and expression. Let's begin by talking about\nAndroid's ecosystem of devices, starting with two of\nthe most important-- tablets and watches. Over the last two years, we've\nredesigned the experience on large screens, including\ntablets and foldables. We introduced a new\nsystem for multitasking that makes it so much\neasier to take advantage of all that extra screen real\nestate and seamlessly move between apps. We've made huge investments to\noptimize more than 50 Google Apps, including Gmail,\nPhotos, and Meet. And we're working\nclosely with partners, such as Minecraft,\nSpotify, and Disney Plus to build beautiful\nexperiences that feel intuitive on larger screens. People are falling in\nlove with Android tablets, and there are more great\ndevices to pick from than ever. Stay tuned for our\nhardware announcements, where you just might see some\nof the awesome new features we're building for\ntablets in action. It's really exciting\nto see the-- [APPLAUSE] It's really exciting to see\nthe momentum in smart watches as well. Wear OS is now the fastest\ngrowing watch platform, just two years after launching\nWear OS 3 with Samsung. A top ask from fans has been\nfour more native messaging apps on the watch. I'm excited to\nshare that WhatsApp is bringing their first-ever\nwatch app to Wear this summer. I'm really enjoying using\nWhatsApp on my wrist. I can start a new conversation,\nreply to messages by voice, and even take calls. I can't wait for you to try it. Our partnership on Wear OS\nwith Samsung has been amazing, and I'm excited\nabout our new Android collaboration on immersive XR. We'll share more\nlater this year. Now, we all know that to\nget the best experience, all these devices need to\nwork seamlessly together. It's got to be simple. That's why we built\nFast Pair, which lets you easily connect\nmore than 300 headphones, and while we have Nearby\nShare to easily move files between your phone,\ntablet, or Windows and Chrome OS computer, and cast\nto make streaming video and audio to your\ndevices ultra simple, with support from\nover 3,000 apps. It's great to have all\nyour devices connected. But if you're\nanything like me, it can be hard to keep\ntrack of all this stuff. Just ask my family. I misplace my earbuds at\nleast three times a day, which is why we're launching\na major update to our Find My Device experience to support\na wide range of devices in your life, including\nheadphones, tablets, and more. It's powered by a network of\nbillions of Android devices around the world. So if you leave your\nearbuds at the gym, other nearby Android devices\ncan help you locate them. And for other important\nthings in your life, like your bicycle or suitcase,\nTile, Chipolo, and others will have tracker tags\nthat work with the Find My device network as well. [APPLAUSE] Now, we took some time\nto really get this right, because protecting your\nprivacy and safety is vital. From the start, we\ndesigned the network in a privacy-preserving way,\nwhere location information is encrypted. No one else can tell where\nyour devices are located, not even Google. This is also why we're\nintroducing unknown tracker alerts. Your phone will tell you\nif an unrecognized tracking tag is moving with you\nand help you locate it. [APPLAUSE] It's important these warnings\nwork on your Android phone, but on other types\nof phones as well. That's why, last week, we\npublished a new industry standard with Apple, outlining\nhow unknown tracker alerts will work across all smartphones. [APPLAUSE] Both the new Find My Device\nexperience and unknown tracker alerts are coming\nlater this summer. Now, we've talked a lot\nabout connecting devices, but Android is also\nabout connecting people. After all, phones\nwere created for us to communicate with\nour friends and family. When you're texting\nin a group chat, you shouldn't have to worry\nabout whether everyone is using the same type of phone. Sending high-quality-- [APPLAUSE] Sending high-quality\nimages and video, getting typing notifications,\nand end-to-end encryption should all just work. That's why we've worked with our\npartners on upgrading old SMS and MMS technology\nto a modern standard called RCS that makes\nall of this possible. And there are now over 800\nmillion people with RCS, on our way to over a billion\nby the end of the year. We hope every mobile operating\nsystem gets the message and adopts RCS-- [APPLAUSE] --so we can all hang out\nin the group chat together, no matter what\ndevice we're using. Whether it's connecting\nwith your loved ones or connecting all\nof your devices, Android's complete\necosystem makes it easy. Another thing people\nlove about Android is the ability to\ncustomize their devices and express themselves. Here's Dave to\ntell you how we're taking this to the next\nlevel with generative AI. [MUSIC PLAYING] [APPLAUSE] DAVE BURKE: All right. Thanks, Sameer, and\nhello, everyone. So here's the thing. People want to\nexpress themselves in the products\nthey use every day, from the clothes they\nwear to the car they drive to their surroundings at home. We believe the same should\nbe true for your technology. Your phone should feel like\nit was made just for you. And that's why\ncustomization has always been at the core of\nthe Android experience. This year, we're combining\nAndroid's guided customization with Google's advances\nin generative AI so your phone can feel\neven more personal. So let me show you\nwhat this looks like. To start, messages\nand conversations can be so much more\nexpressive, fun and, playful with Magic Compose. It's a new feature coming\nto Google messages powered by generative AI\nthat helps you add that extra spark of personality\nto your conversation. So just type your\nmessage like you normally would, and then\nchoose how you want to sound. Magic Compose will do the\nrest so your messages give off more positivity, more\nrhymes, more professionalism, or if you want, in the style\nof a certain playwright. To try or not to try this\nfeature-- that is the question. Now, we also have\nnew personalizations coming to the OS layer. At Google I/O two years ago,\nwe introduced Material You. It's a design system that\ncombines user inspiration with dynamic color science for\na fully personalized experience. We're continuing to expand\non this in Android 14, with all-new customization\noptions coming to your lockscreen. So now, I can add my\nown personalized style to the lockscreen clock so\nit looks just the way I want. And what's more, with the\nnew customizable lockscreen shortcuts, I can instantly\njump into my most frequent activities. Of course, what really makes\nyour lock screen and home screen yours is the wallpaper. And it's the first\nthing that many of us set when we get a new phone. Now, emojis are such\na fun and simple way of expressing yourself. So we thought, wouldn't\nit be cool to bring them to your wallpaper? So with emoji\nwallpapers, you choose your favorite\ncombination of emoji, pick the perfect pattern, and\nthen find just the right color to bring them all together. So let's take a look. And I'm not going\nto use the laptops. I'm going to use a phone. So let's see. I'm going to go into\nthe wallpaper picker and I'm going to tap on\nthe new option for emojis. And I'm feeling it a\nkind of, I don't know, zany mood with all you\npeople looking at me. So I'm going to pick\nthis guy and this guy. And let's see, who\nelse is in here? This one looks pretty cool,\nlike, the eight-bit one. And obviously that one. And somebody said there was\na duck on stage earlier. So let's go find a duck. Hello, duck. Where is the duck? Can anyone see a duck? Where has the duck gone? There is the duck. All right, there he is. I got some ducks. OK, cool. And then pattern-wise, we've got\na bunch of different patterns you can pick. I'm going to pick mosaic. That's my favorite. I'm going to play with the zoom. Let's see. We'll get this just right. OK, I've got enough\nducks in there. OK, cool. And then colors. Let's see. Ooh, that pops. Let's go with a more muted one. Or maybe that one. That one looks good. That looks good. I like that one. All right, select that, set the\nwallpaper, and then I go boom. Looks pretty cool, huh? [APPLAUSE] And the little emojis, they\nreact when you tap them, which I find-- I find this\nunusually satisfying. And how much time have I got? OK, I'll move on. OK, so of course,\nmany of us like to use a favorite photo\nfor our wallpaper. And so with a new cinematic\nwallpaper feature, you can create a stunning 3D\nimage from any regular photo and then use it\nas your wallpaper. So let's take a look. So this time, I'm going\nto go into My Photos. And I really like this\nphoto of my daughter, so let me select that. And you'll notice there's\na Sparkle icon at the top. So if I tap that, I get a new\noption for cinematic wallpaper. So let me activate that. And then wait for it-- boom. Now, under the hood, we're\nusing an on-device convolutional neural network to\nestimate depth and then a generative adversarial\nnetwork for in-painting as the background moves. The result is a beautiful,\ncinematic 3D photo. So then let me\nset the wallpaper, and then I'm going\nto return home. And check out the parallax\neffect as I tilt the device. It literally jumps\noff the screen. So both cinematic wallpapers\nand emoji wallpapers are coming first to\nPixel devices next month. So let's say you don't have the\nperfect wallpaper photo handy, or you just want to have fun\nand create something new. With our new generative\nAI wallpapers, you choose what\ninspires you and then we create a beautiful\nwallpaper to fit your vision. So let's take a look. So this time, I'm\ngoing to go and select Create a Wallpaper with AI. And I like classic art,\nso let me tap that. Now, you'll notice,\nat the bottom, we use structured prompts\nto make it easier to create. So for example, I can pick-- what am I going to do? City by the bay in a post\nimpressionist style cool Then I type-- tap Create Wallpaper. Nice. Now, behind the\nscenes, we're using Google's text-to-image diffusion\nmodels to generate completely new and original wallpapers. And I can swipe through and\nsee all the different options that it's created. And some of these look\nreally cool, right? [APPLAUSE] So let me pick this one. I like this one. So select that,\nset the wallpaper, and then return home. Cool. So now, out of the billions of\nAndroid phones in the world, no other phone will\nbe quite like mine. And thanks to Material You, you\ncan see that the system's color palette is automatically\nadapted to match the wallpaper I created. Generative AI wallpapers\nwill be coming this fall. [APPLAUSE] So from a thriving\necosystem of devices to AI-powered expression,\nthere is so much going on right now in Android. OK, Rick is up next to show you\nhow this Android innovation is coming to life in the\nPixel family of devices. Thank you. [MUSIC PLAYING] [APPLAUSE] RICK OSTERLOH: The pace of AI\ninnovation over the past year has been astounding. As you heard Sundar\ntalk about earlier, new advances are\ntransforming everything, from creativity and productivity\nto knowledge and learning. Now, let's talk about\nwhat that innovation means for Pixel, which\nhas been leading the way in AI-driven hardware\nexperiences for years. Now, from the\nbeginning, Pixel was conceived as an AI-first\nmobile computer, bringing together all\nthe amazing breakthroughs across the company and putting\nthem into a Google device you can hold in your hand. Other phones have AI features,\nbut Pixel is the only phone with AI at the center. And I mean that literally. The Google Tensor\nG2 chip is custom designed to put Google's\nleading-edge AI research to work in our Pixel devices. By combining Tensor's\non-device intelligence with Google's AI\nin the cloud, Pixel delivers truly personal AI. Your device adapts to your\nown needs and preferences, and it anticipates\nhow it can help you save time and get more done. This personal AI enables all\nthose helpful experiences that Pixel is known\nfor that aren't available on any\nother mobile device, like Pixel Call Assist,\nwhich helps you avoid long hold times, navigate\nphone tree menus, ignore the calls you don't want,\nand get better sound quality on the calls you do want. Personally, I also enable\nhelpful Pixel Speech experiences. On-device machine learning\ntranslates different languages for you, transcribes\nconversations in real time, and understands how\nyou talk and type. And you're protected with Pixel\nSafe, a collection of features that keep you safe online\nand in the real world. And of course,\nthere's Pixel Camera. [APPLAUSE] It understands faces,\nexpressions, and skin tones to better depict\nyou and the people you care about so your photos\nwill always look amazing. We're also constantly\nworking to make Pixel camera more inclusive\nand more accessible, with features like Real\nTone and Guided Frame. [APPLAUSE] Pixel experiences\ncontinue to be completely unique in mobile computing. And that's because Pixel is\nthe only phone engineered end-to-end by Google, and\nthe only phone that combines Google Tensor, Android, and AI. [APPLAUSE] With this combination of\nhardware and software, Pixel lets you experience all\nthose incredible new AI-powered features you saw\ntoday in one place. For example, the new\nMagic Editor and Google Photos that Sundar\nshowed you, it'll be available for early access\nto select Pixel phones later this year, opening up a whole\nnew avenue of creativity with your photos. And Dave just showed you\nhow Android's adding depth to how you can express yourself\nwith generative AI wallpapers. And across Search,\nWorkspace, and Bard, new features powered by\nlarge language models can Spark your imagination,\nmake big tasks more manageable, and help you find better answers\nto everyday questions, all from your Pixel device. We have so many more exciting\ndevelopments in the space, and we can't wait to show you\nmore in the coming months. Now, it's probably no\nsurprise that as AI keeps getting more\nand more helpful, our Pixel portfolio keeps\ngrowing in popularity. Last year's Pixel devices are\nour most popular generation yet with both users\nand respected reviewers and analysts. [APPLAUSE] Our Pixel phones won multiple\nPhone of the Year awards. [APPLAUSE] Yes, thank you. And in the premium\nsmartphone category, Google is the fastest\ngrowing OEM in our markets. [APPLAUSE] One of our more popular\nproducts is the Pixel a series, which delivers incredible-- [APPLAUSE] Thank you. I'm glad you like it. It delivers incredible\nPixel performance in a very affordable device. And to continue\nthe I/O tradition, let me show you the newest\nmember of our a series. [APPLAUSE] Today, we're\ncompletely upgrading everything you love\nabout our a series with the gorgeous new Pixel 7a. Like all Pixel 7 series\ndevices, Pixel 7a is powered by our flagship\nGoogle Tensor G2 chip, and it's paired with 8\ngigabytes of RAM, which ensures Pixel 7a delivers\nbest in class performance and intelligence. And you're going\nto love the camera. The 7a takes the crown from\n6A as the highest-rated camera in its class, with the\nbiggest upgrade ever to our a-series camera\nhardware, including a 72% bigger main camera sensor. [APPLAUSE] Now, here's the best part. Pixel 7a is available\ntoday, starting at $499. It's an unbeatable combination\nof design, performance, and photography, all\nat a great value. And you can check out the\nentire Pixel 7a lineup on the Google Store, including\nour exclusive coral color. Now, next up, we're\ngoing to show you how we're continuing to\nexpand the Pixel portfolio into new form factors. Yeah. [APPLAUSE] Like foldables and tablets. You can see them right there. It's a complete ecosystem of\nAI-powered devices engineered by Google. Here's Rose to show you what\na larger-screen Pixel can do for you. [MUSIC PLAYING] [APPLAUSE] ROSE YAO: OK, let's\ntalk tablets, which have been a little bit frustrating. It's always hard to\nknow where they fit in, and they haven't really\nchanged in the past 10 years. A lot of times, they are\nsitting forgotten in the drawer, and that one moment you need\nit, it is out of battery. We believe tablets, and\nlarge screens in general, still have a lot of potential. So we set out to build\nsomething different, making big investments across\nGoogle Apps, Android, and Pixel to reimagine how large\nscreens can deliver a more helpful experience. Pixel Tablet is the only\ntablet engineered by Google and designed specifically\nto be helpful in your hand and in the place they\nare used the most-- the home. We designed the Pixel\nTablet to uniquely deliver helpful Pixel experiences. And that starts with\ngreat hardware-- a beautiful, 11-inch,\nhigh-resolution display with crisp audio from the four\nbuilt-in speakers, a premium aluminum enclosure with\na nanoceramic coating that feels great in the hand\nand is cool to the touch. The world's best Android\nexperience on a tablet powered by Google Tensor G2\nfor long-lasting battery life and cutting-edge personal AI. For example, with Tensor G2,\nwe optimized the Pixel camera specifically for video calling. Tablets are fantastic\nvideo calling devices, and with Pixel Tablet, you\nare always in frame, in focus, and looking your best. The large screen makes Pixel\nTablet the best Pixel device for editing photos, with\nAI-powered tools like Magic Eraser and Photo Unblur. Now, typing on a tablet\ncan be so frustrating. With Pixel Speech\nand Tensor G2, we have the best voice\nrecognition, making voice typing nearly three\ntimes faster than tapping. And as Sameer mentioned, we've\nbeen making huge investments to create great app experiences\nfor larger screens, including more than 50 of our own apps. With Pixel Tablet, you're\ngetting great tablet hardware with great tablet apps. But we saw an opportunity\nto make the tablet even more helpful in the home. So we engineered a\nfirst-of-its-kind charging speaker dock. [APPLAUSE] It gives the tablet\na home, and now you never have to worry\nabout being charged. Pixel Tablet is always\nready to help, 24/7. When it's docked,\nthe new Hub mode turns Pixel Tablet into a\nbeautiful digital photo frame, a powerful smart\nhome controller, a voice-activated helper, and\na shared entertainment device. It feels like a smart display,\nbut has one huge advantage-- with the ultra-fast\nfingerprint sensor, I can quickly unlock the device\nand get immediate access to all my favorite Android apps. So I can quickly find the\nrecipe with Side Chef, or discover a new\npodcast on Spotify, or find something to watch with\na tablet-optimized Google TV app. Your media is going\nto look and sound great, with room-filling sound\nfrom the charging speaker dock. Pixel Tablet is also\nthe ultimate way to control your\nsmart home, and that starts with a new\nredesigned Google Home app. It looks great on Pixel\nTablet, and it brings together over 80,000 supported smart\nhome devices, including all of your Matter-enabled devices. We also-- [APPLAUSE] We also made it really easy to\naccess your smart home controls directly from Hub mode. With the new home\npanel, any family member can quickly adjust the\nlights, lock the doors, or see if a package\nwas delivered. Or if you're lazy like me,\nyou can just use your voice. Now, we know that\ntablets are often shared, so a tablet for the home needs\nto support multiple users. Pixel Tablet makes switching\nbetween users super easy. So you get your own apps\nand your own content while maintaining your privacy. [APPLAUSE] And my favorite part-- it is so easy to move\ncontent between devices. Pixel Tablet is the first\ntablet with Chromecast built in. So with a few taps-- [APPLAUSE] --I can easily cast some\nmusic or my favorite show from my phone to the tablet. And then I can just take\nthe tablet off the dock and keep listening or\nwatching all around the house. We designed a new type\nof case for Pixel Tablet that solves the pain\nof flimsy tablet cases. It has a built-in stand that\nprovides continuous flexibility and is sturdy at all angles\nso you can comfortably use your tablet\nanywhere-- on the plane, in bed, or in the kitchen. The case easily docks. You never have to\ntake it off to charge. And it's just another example\nof how we can make the tablet experience even more helpful. [APPLAUSE] The new Pixel Tablet\ncomes in three colors. It is available for\npre-order today, and shifts next month\nstarting at just $499. [APPLAUSE] And the best part-- every Pixel Tablet comes bundled\nwith the 129 charging speaker dock for free. [APPLAUSE] It is truly the best tablet\nin your hand and in your home. To give you an idea just how\nhelpful Pixel Tablet can be, we asked TV personality Michelle\nButeau to put it to the test. Let's see how that went. [VIDEO PLAYBACK] [MUSIC PLAYING] - When Google asked me to\nspend the day with this tablet, I was a little apprehensive,\nbecause I'm not a tech person. I don't know how things\nwork all the time. But I'm a woman in STEM now. Some days, I could\nbarely find the floor, let alone a charger\nfor something. So when the Google\nfolks said something about a tablet that docks, I was\nlike, OK, that Google proofing. I am, on average-- two to five meetings a day. Today, I got stuck on all these\nfeatures, honey-- the 360 of it all. The last time I was around\nthis much sand, some of it got caught in my belly\nbutton, and I had a pearl two weeks later. Look, it's a bird! So this is what I loved\nabout my me time today. Six shows just popped up\nbased off of my preferences. And they were like, hey, girl. That would have made it\nfunnier, but that was good. My husband is actually\na photographer, so I have to rely on him to\nmake everything nice and pretty. But now-- I love this\npicture of me and my son, but there's a boom mic there. Look, it's right here. You see this one? Get this mic. You see that? Magic Eraser. Take a circle or brush-- I'm going to do both. Boom! How cute is that? And so I hope not only\nyou guys are happy with me reviewing this,\nbut that you'll also give me one, because, I mean. You're getting tired, right? - I'm not. - You're not? OK, because I am. [END PLAYBACK] [APPLAUSE] RICK OSTERLOH: That's a\npretty good first review. Now, tablets aren't the\nonly large-screen device we want to show you today. It's been really exciting\nto see foldables take off over the past few years. Android's driven\nso much innovation in this new form factor, and we\nsee tremendous potential here. We've heard from our users\nthat the dream foldable should have a versatile\nform factor, making it great to use both\nfolded and unfolded. It should also have a\nflagship-level camera system that truly takes advantage of\nthe unique design and an app experience that's fluid and\nseamless across both screens. Creating a foldable like\nthat, it really means pushing the envelope with\nstate-of-the-art technology, and that means an ultra\npremium $1799 device. Now, to get there,\nwe've been working closely with our\nAndroid colleagues to create a new standard\nfor foldable technology. Introducing Google Pixel Fold. [APPLAUSE] It combines Tensor G2,\nAndroid innovation, and AI for an\nincredible phone that unfolds into an\nincredible compact tablet. It's the only foldable\nengineered by Google to adapt to how\nyou want to use it, with a familiar front\ndisplay that works great when it's folded, and\nwhen it's unfolded, it's our thinnest phone yet,\nand the thinnest foldable on the market. Now, to get there, we had to\npack a flagship-level phone into nearly half\nthe thickness, which meant completely\nredesigning and components like the telephoto lens, and\nthe battery, and a lot more. So it can fold up and it\ncan fit in your pocket and retain that familiar\nsmartphone silhouette when it's in your hand. The Pixel fold has three\ntimes the screen space of a normal phone. You unfold it, and you're\ntreated to an expansive 7.6-inch display that opens flat\nwith a custom 180-degree fluid friction hinge. So you're getting the\nbest of both worlds. It's a powerful smartphone\nwhen it's convenient and an immersive tablet\nwhen you need one. And like every phone we make,\nPixel Fold is built to last. We've extensively\ntested the hinge to be the most durable\nof any foldable. Corning Gorilla Glass\nVictus protects it from exterior scratches, while\nthe IPX8 water-resistant design safeguards against the weather. And as you'd expect\nfrom a Pixel device, Pixel Fold gives you\nentirely new ways to take stunning photos and\nvideos with Pixel camera. You put the camera in Tabletop\nmode to capture the stars, and you can get closer with\nthe best zoom on a foldable. And use the best camera on\nthe phone for your selfies. The unique combination of form\nfactor, triple rear camera hardware, and personal\nAI with Tensor G2 make it the best\nfoldable camera system. [APPLAUSE] Now, there are so\nmany experiences that feel even more natural\nwith the Pixel Fold. One is the dual screen\ninterpreter mode. Your Pixel Fold-- [APPLAUSE] Your Pixel Fold can\nuse both displays-- both displays-- to\nprovide a live translation to you and the person\nyou're talking to. So it's really easy to\nconnect across languages. [APPLAUSE] And powering all of this\nis Google Tensor G2. Pixel Fold has all the personal\nAI features you'd expect from a top-of-the-line Pixel device,\nincluding safety, speech, and call assist. Plus, it's got great performance\nfor on-the-go multitasking and entertainment. And the entire foldable\nexperience is built on Android. So let's get Dave back out\nhere to show you the latest improvements to\nAndroid you'll get to experience on a Pixel Fold. DAVE BURKE: All right. Thanks, Rick. From new form factors\ncustomizability to biometrics and\ncomputational photography, Android has always been at the\nforefront of mobile industry breakthroughs. And recently, we've been\nworking on a ton of features and improvements for\nlarge-screen devices like tablets and foldables. So who thinks we should\ntry a bunch of live demos on the new Pixel Fold? [APPLAUSE] All right, let's do it. It starts the second\nI unfold the device, with this stunning\nwallpaper animation. And the hinge sensor is\nactually driving the animation. And it's a subtle thing,\nbut it makes the device feel so dynamic and alive. Yeah, I just love that. So let's go back to\nthe folded state. And I'm looking at Google Photos\nof a recent snowboarding trip. Now, the scenery is\nreally beautiful. So I want to show you\non the big screen. I just open my phone,\nand the video instantly expands into this\ngorgeous, full-screen view. We call this feature\ncontinuity, and we've obsessed over every\nmillisecond it takes for apps to seamlessly\nadapt from the small screen to the larger screen. Now, all work and no play\nmakes Davy a dull boy, so I'm going to message\nmy buddy about getting back out on the mountain. I can just swipe to bring up\nthe new Android taskbar, then drag Google Messages to the\nside to enter split screen mode like so. To inspire my buddy, I'm\ngoing to send them a photo. So I can just drag and\ndrop from Google Photos right into my message, like so. And thanks to the new Jetpack\ndrag and drop library, this is now supported in\na wide variety of apps, from Workspace to WhatsApp. You'll notice we've made\na bunch of improvements throughout the OS to take\nadvantage of the larger screen. So for example, here's\nthe new split keyboards for faster typing. And if I pull down\nfrom the top, you'll notice the new two-panel shade\nshowing both my notifications and my quick settings\nat the same time. Now, Pixel Fold is great\nfor productivity on the go. And if I swipe up\ninto Overview, you'll notice that we now keep the\nmultitasking windows paired. And for example, I was\nworking on a Google Docs and Slides earlier to\nprep for this keynote. And I think I'm following\nmost of these tips so far, but I'm not quite done yet. I've been warned, by the way. Anyway, I could even\nadjust the split to suit the content\nthat I'm viewing. And working this way, it's like\nhaving a dual monitor set up in the palm of my hand, allowing\nme to do two things at once-- which reminds me,\nI should probably send Rick a quick note. So I'll open Gmail. And I don't have a\nlot of time, so I'm going to use the new\nhelp me write feature. So let's try this out. [APPLAUSE] Don't cheer yet. Let's see if it works. OK, Rick-- Rick-- congrats on-- what are\nwe going to call this? Pixel Fold's launch. Amazing with Android. And then I probably should say-- not Andrew, Android. Dave. It's hard to type with all\nyou people looking at me. All right. Now, by the power of\nlarge language models, allow me to elaborate. Dear Rick, congratulations\non the successful launch of Pixel Fold. I'm really impressed\nwith the device and how well it\nintegrates Android. The foldable screen\nis a game changer and I can't wait to see\nwhat you do with it now. [APPLAUSE] That's productivity. But there's more. The Pixel Fold is also an\nawesome entertainment device, and YouTube is just a really\ngreat showcase for this. So let's start watching this\nvideo on the big screen. Now, look what\nhappens when I fold the device at right angles. YouTube enters what\nwe call Tabletop mode so that the video\nplays on the top half, and then we're working on\nadding playback controls to the bottom half for\nan awesome single-handed lean-back experience. And the video just\nkeeps playing fluidly through these transitions\nwithout losing a beat. One last thing. We're adding support\nfor switching displays from within an app. And Pixel Fold's camera is a\nreally great example of that. Now, by the way, say\nhi to Julie behind me. She's the real star of the show. So Pixel Fold has this new\nbutton on the bottom right. So I'm going to tap this. And it means I can\nmove the viewfinder to the outside screen. So let me turn\nthe device around. OK, so why is this interesting? Well, it means that\nthe viewfinder is now beside the rear camera system. And that means I can get a\nhigh-quality, ultra-wide, amazing selfie with the\nbest camera on the device. Speaking of which-- and you\nknew where this was going-- smile, everybody! You look awesome. Woo-hoo! I always wanted to do that\nat a Google I/O keynote. So what you're seeing\nhere is the culmination of several years of work,\nin fact, on large screens, spanning the Android OS\nand the most popular apps on the Play Store. All this work comes alive\non The Amazing new Pixel Tablet and Pixel Fold. Check out this video. Thank you. [VIDEO PLAYBACK] [MUSIC PLAYING] [END PLAYBACK] [APPLAUSE] RICK OSTERLOH: Whoo! That demo was awesome. Across Pixel and Android,\nwe're making huge strides with large-screen devices. And we can't wait to get\nPixel Tablet and Pixel Fold into your hands. And you're not going to\nhave to wait too long. You can pre-order Pixel\nFold starting today, and it'll ship next month. And you'll get the most out\nof our first ultra premium foldable by pairing\nit with Pixel Watch. So when you pre-order\na Pixel Fold, you'll also get a\nPixel Watch on us. [APPLAUSE] The Pixel family\ncontinues to grow into the most dynamic\nmobile hardware portfolio in the market today. From a broad selection\nof smartphones to watches, earbuds, and\nnow, tablets and foldables, there are more ways\nthan ever to experience the helpfulness Pixel is known\nfor wherever and whenever you need it. Now, let me pass\nit back to Sundar. Thanks, everyone. [MUSIC PLAYING] [APPLAUSE] SUNDAR PICHAI: Thanks, Rick. I'm really enjoying the new\ntablet and the first Pixel foldable phone, and I'm proud\nof the progress Android is driving across the ecosystem. As we wrap up, I've\nbeen reflecting on the big technology shifts\nthat we've all been a part of. The shift with AI is\nas big as they come. And that's why it's\nso important that we make AI helpful for everyone. We are approaching it boldly,\nwith a sense of excitement, because as we look ahead,\nGoogle's deep understanding of information, combined\nwith the capabilities of generative AI, can transform\nSearch and all of our products yet again. And we are doing\nthis responsibly in a way that underscores\nthe deep commitment we feel to get it right. No one company\ncan do this alone. Our developer community\nwill be key to unlocking the enormous\nopportunities ahead. We look forward to working\ntogether and building together. So on behalf of all\nof us at Google, thank you and enjoy\nthe rest of I/O. [APPLAUSE] [MUSIC PLAYING] CHLOE: Well, that's a wrap\non the Google keynote. We've got much more\nI/O content coming up. So before you take a\nbreak or grab a snack, you'll want to make sure\nyou're right back here in just a few minutes. CRAIG: That's right. I'm Craig and she's\nChloe, and we'll be showing off I/O\nFlip, a classic card game with an AI twist. We released the game\nyesterday to show you what's possible with generative\nAI and other Google tools like Flutter, Firebase,\nand Google Cloud. So come see how\nthe game is played or play along with us at home. For now, check out our\nnew I/O Flip game trailer. [MUSIC PLAYING] [VIDEO PLAYBACK] - My name is Irem. I'm an engineer at\nGoogle, and I've been working on Project Starline\nfor a little over a year. Project Starline's mission is to\nbring people together and make them feel present\nwith each other even if, physically,\nthey are miles apart. An earlier prototype relied\non several cameras and sensors to produce a live 3D image\nof the remote person. In our new prototype, we have\ndeveloped a breakthrough AI technique that learns\nto create a person's 3D likeness and image using\nonly a few standard cameras. - I'm Melanie Lowe, and\nI'm the global workplace design lead here at Salesforce. You were so used to seeing a\ntwo-dimensional little box, and then we're\nconnecting like this. And that feeling of being\nin front of a person is now replicated in Starline. - I'm more than happy\nto be part of the setup. I was kind of curious\nabout the collaboration. I was like, is this possible? - You felt like someone\nwas right there. - Thanks for having me. Yeah, of course; - The first meeting\nI had on Starline. I said, wow, you've\ngot blue eyes. And this is the person I'd\nbeen meeting with for a year. Just to see a person in 3D,\nit was really astounding. - His smile was the\nsame smile, exactly how when I first met him. - Oh, Elaine is in\nAtlanta, but actually, it feels like she's\nsitting in front of me. - Starline is really about\nthe person you're talking to. All the technology sort\nof falls by the wayside. [END PLAYBACK] [MUSIC PLAYING] [VIDEO PLAYBACK] - As a Black woman in tech,\nno matter what, I'm Black. - Think about all\nof the technologies that we use all the\ntime and how few of them are designed from\na headspace that considers an identity like\nmine, like Black people. - I provide Material\nfor Black artists to create authentic depictions\nof their own community. - You have to use\nyour imagination. How do you build\nyour own technology? - I want to challenge what\npeople want and provide something new. [END PLAYBACK] [MUSIC PLAYING] [VIDEO PLAYBACK] - Turn, turn, turn, drop. - There's no way to\nchange somebody's life more than to give\nthem a good education. 7 Generation Games makes games\nand the tools to make them. - We started focused\non closing the math gap in underserved communities. You talk about, how do we get\nkids engaged, make it relevant? Because it's really\npowerful when they see someone\nwho looks like them reflected the very first\ntime on their device. - Music was always\npart of our life. - When I worked on my previous\njob, I injured my back. - During his rehabilitation,\nhe started his long walks. And he wanted to\nlisten to music. - I started to develop equalizer\napp, and this is the story. - The app allows you to have\nyour own music experience, to hear the music you\nlike the way you like. - I started my career\nas a historian. Game developer was\nnever the plan. But then games came to my life. And I started to combine\nhistory and entertainment to touch people's hearts. - Our first game, [INAUDIBLE],,\nis influenced by a conflict that happened in [INAUDIBLE],,\nthe Badlands of Bahia, in the 19th century. I think the game\ncould be a good way to make this memorable\nfor many generations. - You cannot imagine how\nliberated I feel no longer being defined by my struggle. It was only when I realized\nthat with the right tools, with the right training,\nstuttering is something that I could control. --of a particular model. - Access to speech therapy\nis a global problem, but it's particularly acute\nin the developing countries. - [INAUDIBLE] we realized we\nneed to code this in to an app and then maybe you can help\nother people [INAUDIBLE].. [END PLAYBACK] [MUSIC PLAYING] CHLOE: Hi, I'm Chloe. CRAIG: And I'm Craig. And we're super excited\nto introduce you to I/O Flip, a classic card game\nwith a modern AI twist, powered by Google, built for I/O,\nand featuring a number of our favorite products. CHLOE: We just released it\nyesterday, and a lot of you have already checked it\nout at flip.withgoogle.com. If you haven't,\nmake sure you do. For now, we're here to give you\na real-time demo and details about some of the tech we\nuse, like Flutter, Firebase, and Google Cloud. You'll want to stay tuned for\nthe developer keynote coming up after this to see how\nthe game was made. CRAIG: For this\ndemo, Chloe and I, well, we happen to know the\nfolks that made this game. Hey, Flip team. So we're able to play against\neach other just for today. And the winner gets to take home\nthat trophy right over there. AUDIENCE: Wow! CHLOE: I know the perfect\nplace for that trophy. CRAIG: Chloe, how do you know\nwhat my trophy case looks like? CHLOE: OK. So to get started\non IO Flip, you get to build your\nown team, customizing your cards with classes and\nspecial powers along the way. And there are some extra bonuses\nthat add to your strength, like holographic cards\nand elemental powers. More on those later. You win when your cards are more\npowerful than your opponent's cards. All right, Craig. We're going to try this out on\nour new Pixel 7a's right here. Are you ready to open a pack\nand start building our teams? CRAIG: Let's do it. Let's play I/O Flip. [MUSIC PLAYING] CHLOE: OK. First, we're going to build\nan AI-designed pack of cards featuring some of our beloved\nGoogle characters, Dash, Sparky, Android, and Dino. Then we're going to\nassign them special powers and see what we get. Let's see. What do I want for my team? Ooh, I'm going to go with\nfairy, because an army of pixies sounds like a crew that\nI want to hang out with. And let's see. For my special power, I'm\ngoing to choose breakdancing, because nothing is more powerful\nthan the power of dance. CRAIG: All right. Well, I've chosen\npirate as my class. And Chloe, you do\nknow how to tell if someone's a pirate, right? Well, they're always\ntalking about plunder. All right, so now I\nneed a special power. Let's see, astrology. If only that had been\nastronomy, the pirates might have actually\nbeen able to use it. Let's see. Break dancing pirates? What is this,\n\"Pirates of Penzance\"? Oh fake. Crying that's good. Be careful, Chloe. My pirates are also good\nat emotional manipulation. They've never seen a guilt\ntrip they wouldn't take. CHLOE: Fake crying? Huh, I didn't realize\nthat I had a superpower. We each get 12 cards in a\npack, and from here, we'll be able to swipe through\nand strategize and decide which three we think will be\nour strongest competitors. Those three become\nour teams, and they're the cards that will compete\nwith our opponent's team. CRAIG: Oh, here's\na good description. Sparky the pirate fake\ncries to get out of trouble, but he always laughs it off. Pretty childish, Sparky, but\nalso a pretty good flavor text. Now, Maker suite helped us\nprototype all of those prompts, and then the PaLM API\ngenerated thousands of unique descriptions\nfor all these cards. CHLOE: And those animations\nare silky smooth with Flutter. And what's even cooler is that\nbecause those animations are powered by code and\nnot video assets, they're really flexible. And even more, because all\nof this is made with Flutter, we don't only have\na web app here. We're most of the way toward a\nmobile app on Android and iOS as well. And to give you a peek\nbehind the scenes here, all the images were created\nwith Google AI tools. We're committed to using AI\nresponsibly here at Google. So we collaborated with\nartists to train our AI models, which then generated\nthe thousands of images that you see in I/O Flip. CRAIG: All the game\nplay communication, like matchmaking\nand results, was easy to implement\nwith Firestore. And with Cloud Run, we were\nable to deploy and scale our all-Dart back end. That's right, I/O Flip\nis fullstack Dart. OK, Chloe. I think it's time to flip. CHLOE: OK. Now, this is a fast-paced game. Things happen pretty quickly. So pay attention. CRAIG: OK, we're in. CHLOE: OK, I've got my card. All right, fairies,\nlet's break dance. CRAIG: All right, pirates. Yarr. OK, moment of truth here. The answer is-- oh. And your water elemental\npower further beating my fire, as if you even needed it. OK, round two. Pressure is on. Think I've got a winner. CHLOE: We'll see about that. Right into my trap. CRAIG: Oh, these are\nreal tears, Chloe. CHLOE: That is my lowest card. CRAIG: Whoo, this is\nfor all the marbles. CHLOE: I feel good\nabout this one. I think I'm going\nto get that trophy. CRAIG: Me too. Me too. Wait-- fire's melting your\nmetal, but not enough. Chloe, you've taken it. Well-played, Chloe. I suppose if anyone deserved\nthat trophy other than me, it would be you. CHLOE: Thanks, Craig. Once again, the power\nof dance prevails. Super fun, super easy to play. I love being able to customize\nmy cards and characters. And I can play quick games\nif I'm short on time, or I can play again to\nextend my winning streak. CRAIG: Want to play\nI/O Flip yourself? Go to flip.withgoogle.com to\nplay on your laptop, desktop, or mobile device. CHLOE: Thanks for\nhanging out with us and checking out I/O Flip. And you can learn more about\nthe AI technology actually used to make the game and so\nmuch more coming up next in the developer keynote. CRAIG: See you there. [MUSIC PLAYING]"}]}}